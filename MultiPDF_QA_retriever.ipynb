{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "437c7d4c-f1f5-42e2-835b-330a54618623",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pysqlite3 pysqlite3-binary pypdf --q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f1b5bb2-1a9c-43be-9973-a58f4ffd319b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the necessary libraries\n",
    "\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import TextLoader, PyPDFLoader, DirectoryLoader\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "import os\n",
    "import openai\n",
    "\n",
    "__import__('pysqlite3')\n",
    "\n",
    "import sys\n",
    "sys.modules['sqlite3'] = sys.modules['pysqlite3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8eed1aff-001a-46f1-9630-91b01369c5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading and splitting all the documents from the folder GenAI papers\n",
    "\n",
    "#Loading the documents\n",
    "doc_loader = DirectoryLoader('GenAI_Papers', glob=\"./*.pdf\", loader_cls=PyPDFLoader)\n",
    "documents = doc_loader.load()\n",
    "#Splitting the documents\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "texts = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8cf7cfe3-13ef-4379-a42d-4ed8e1d5aaeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the OpenAI embeddings\n",
    "\n",
    "openai_embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd9aaf2b-8464-486b-a175-cccdb9e230df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Pdf_Summarizer.ipynb',\n",
       " '.gitconfig',\n",
       " 'the_nestle_hr_policy_pdf_2012.pdf',\n",
       " 'Nestle_Hr_chatbot.ipynb',\n",
       " '.config',\n",
       " '.gradio',\n",
       " '.ssh',\n",
       " 'SUMMARY.txt',\n",
       " '.cache',\n",
       " 'Untitled1.ipynb',\n",
       " '.ipynb_checkpoints',\n",
       " 'arxiv_impact_of_GENAI.pdf',\n",
       " '.voc',\n",
       " 'DesignGeneration.ipynb',\n",
       " 'GenAI_Papers',\n",
       " '.jupyter',\n",
       " '.ipython',\n",
       " 'Untitled.ipynb',\n",
       " '.local',\n",
       " 'nestle_hr_policy']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb1d22cd-fbdc-416f-bddc-062dd44fd27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating and loading the database\n",
    "\n",
    "#Creating a database\n",
    "persist_directory = 'db'\n",
    "embedding = openai_embeddings\n",
    "vectordb = Chroma.from_documents(documents=texts,\n",
    "                                 embedding=embedding,\n",
    "                                 persist_directory=persist_directory)\n",
    "vectordb.persist()\n",
    "vectordb = None\n",
    "#Loading the database\n",
    "vectordb = Chroma(persist_directory=persist_directory,\n",
    "                  embedding_function=embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3f29eab-bd79-4ff7-b53f-3bd1950ec4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating and using the retriever\n",
    "\n",
    "#Creating retriever\n",
    "retriever = vectordb.as_retriever()\n",
    "#Using retriever\n",
    "docs = retriever.get_relevant_documents(\"What is toolformer?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b43f8b03-a937-4066-8fd4-390436086ff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "with size, measured by the number of trainable parameters: f or example, Wei et al. (2022b ) demonstrate\n",
      "that LLMs become able to perform some BIG-bench tasks3via few-shot prompting once a certain scale is\n",
      "attained. Although a recent line of work yielded smaller LMs that retain some capabilities from their largest\n",
      "counterpart ( Hoï¬€mann et al. ,2022), the size and need for data of LLMs can be impractical for tra ining\n",
      "but also maintenance: continual learning for large models r emains an open research question ( Scialom et al. ,\n",
      "2022). Other limitations of LLMs are discussed by Goldberg (2023) in the context of ChatGPT , a chatbot\n",
      "built upon GPT3 .\n",
      "We argue these issues stem from a fundamental defect of LLMs: they are generally trained to perform\n",
      "statistical language modeling given (i) a single parametri c model and (ii) a limited context, typically the n\n",
      "previous or surrounding tokens. While nhas been growing in recent years thanks to software and hardw are\n"
     ]
    }
   ],
   "source": [
    "query = \"A fundamental limitation of HMMs\"\n",
    "docs = vectordb.similarity_search(query)\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2176cd75-511c-4d0b-823f-3b6458077b9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 [3.10]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
